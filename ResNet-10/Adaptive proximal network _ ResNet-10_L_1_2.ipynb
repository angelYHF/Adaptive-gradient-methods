{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"LxXHXjZxshpj"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"AQaP8QbCxjcI"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PJ4U8cYosloG"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split, KFold\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import mean_squared_error, r2_score\n","from hyperopt import hp, fmin, tpe, Trials, STATUS_OK\n","from joblib import Parallel, delayed\n","import os\n","from scipy.stats import pearsonr\n","\n","# ----------------------------\n","# Proximal operator for L0.5\n","# ----------------------------\n","def prox_half(v, alpha):\n","    \"\"\"\n","    Proximal operator for the L0.5 regularization.\n","\n","    Args:\n","        v (torch.Tensor): The vector to apply the proximal operator to.\n","        alpha (float or torch.Tensor): The regularization parameter.\n","\n","    Returns:\n","        torch.Tensor: The result of the proximal operation.\n","    \"\"\"\n","    if not isinstance(alpha, torch.Tensor):\n","        alpha = torch.tensor(alpha, device=v.device, dtype=v.dtype)\n","\n","    x = torch.abs(v)\n","    thr = (3/2 * alpha) ** (2/3)\n","    mask = x > thr\n","    w = torch.zeros_like(v)\n","\n","    # Check if the mask is not empty before indexing\n","    if mask.any():\n","        v_masked = v[mask]\n","        x_masked = x[mask]\n","\n","        # Corrected proximal operator for L0.5\n","        term = (alpha / 4) * (x_masked) ** (-1.5)\n","        w[mask] = (2/3) * v_masked * (1 + torch.cos(\n","            2/3 * np.pi - (2/3) * torch.acos(torch.clamp(term, -1, 1))\n","        ))\n","\n","    return w\n","\n","class ProxGEN(optim.Optimizer):\n","    \"\"\"\n","    Adaptive proximal gradient optimizer (based on Adam) for various regularizers.\n","    \"\"\"\n","    def __init__(self, params, lr=1e-3, lam=1e-6, betas=(0.1, 0.999), eps=1e-8):\n","        defaults = dict(lr=lr, lam=lam, betas=betas, eps=eps)\n","        super(ProxGEN, self).__init__(params, defaults)\n","\n","    @torch.no_grad()\n","    def step(self, closure=None):\n","        loss = None\n","        if closure is not None:\n","            loss = closure()\n","\n","        for group in self.param_groups:\n","            for p in group[\"params\"]:\n","                if p.grad is None:\n","                    continue\n","                grad = p.grad.data\n","                state = self.state[p]\n","\n","                if len(state) == 0:\n","                    state[\"step\"] = 0\n","                    state[\"exp_avg\"] = torch.zeros_like(p)\n","                    state[\"exp_avg_sq\"] = torch.zeros_like(p)\n","\n","                exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n","                beta1, beta2 = group[\"betas\"]\n","\n","                state[\"step\"] += 1\n","                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n","                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n","\n","                denom = exp_avg_sq.sqrt().add_(group[\"eps\"])\n","                step_size = group[\"lr\"]\n","\n","                theta_hat = p.data.addcdiv(exp_avg, denom, value=-step_size)\n","\n","                # Proximal step for L_0.5\n","                p.data = prox_half(theta_hat, group[\"lam\"] * step_size)\n","\n","        return loss\n","\n","# ----------------------------\n","# ResNet-10 Definition (1D)\n","# ----------------------------\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","    def __init__(self, in_channels, out_channels, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3,\n","                               stride=stride, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm1d(out_channels)\n","        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3,\n","                               stride=1, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm1d(out_channels)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_channels != out_channels:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv1d(in_channels, out_channels, kernel_size=1,\n","                          stride=stride, bias=False),\n","                nn.BatchNorm1d(out_channels),\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_tasks=7):\n","        super(ResNet, self).__init__()\n","        self.in_channels = 64\n","        self.conv1 = nn.Conv1d(1, 64, kernel_size=3, stride=1,\n","                               padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm1d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","\n","        self.linear = nn.Linear(256 * block.expansion, num_tasks)\n","\n","    def _make_layer(self, block, out_channels, num_blocks, stride):\n","        strides = [stride] + [1] * (num_blocks - 1)\n","        layers = []\n","        for s in strides:\n","            layers.append(block(self.in_channels, out_channels, s))\n","            self.in_channels = out_channels * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.layer1(out)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = F.adaptive_avg_pool1d(out, 1)\n","        out = out.view(out.size(0), -1)\n","        out = self.linear(out)\n","        return out\n","\n","def ResNet10(num_tasks=7):\n","    # Custom ResNet-10 configuration with 1, 1, and 1 blocks.\n","    return ResNet(BasicBlock, [1, 1, 1], num_tasks)\n","\n","# ----------------------------\n","# Metrics Calculation\n","# ----------------------------\n","def calculate_sparsity(model):\n","    \"\"\"Calculates the sparsity of the model's weights.\"\"\"\n","    total_params = 0\n","    non_zero_params = 0\n","    for param in model.parameters():\n","        total_params += param.numel()\n","        non_zero_params += torch.count_nonzero(param.data).item()\n","\n","    if total_params == 0:\n","        return 0.0\n","\n","    sparsity = (1 - (non_zero_params / total_params)) * 100\n","    return sparsity\n","\n","def calculate_correlations(labels, predictions):\n","    \"\"\"Calculates the Pearson correlation coefficient for each trait.\"\"\"\n","    num_tasks = labels.shape[1]\n","    correlations = []\n","    for i in range(num_tasks):\n","        corr, _ = pearsonr(labels[:, i], predictions[:, i])\n","        correlations.append(corr)\n","    return correlations\n","\n","# ----------------------------\n","# Load Data\n","# ----------------------------\n","data = pd.read_csv(\"/content/drive/My Drive/Adaptive gradient method/Adaptive gradient method from L_0 to L infnity/Final_pine_data.csv\")\n","Y = data.iloc[:, :7].values  # regression tasks\n","X = data.iloc[:, 7:].values  # SNPs/features\n","\n","scaler = StandardScaler()\n","X = scaler.fit_transform(X)\n","\n","# Split into train+val and test\n","X_trainval, X_test, Y_trainval, Y_test = train_test_split(\n","    X, Y, test_size=0.2, random_state=42\n",")\n","\n","# ----------------------------\n","# Training + CV Evaluation\n","# ----------------------------\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","if torch.cuda.device_count() > 1:\n","    print(f\"Using {torch.cuda.device_count()} GPUs for parallel processing.\")\n","else:\n","    print(\"Using a single GPU or CPU.\")\n","\n","def train_and_eval(X_train, Y_train, X_val, Y_val, params):\n","    train_ds = torch.utils.data.TensorDataset(\n","        torch.tensor(X_train, dtype=torch.float32).unsqueeze(1),\n","        torch.tensor(Y_train, dtype=torch.float32),\n","    )\n","    val_ds = torch.utils.data.TensorDataset(\n","        torch.tensor(X_val, dtype=torch.float32).unsqueeze(1),\n","        torch.tensor(Y_val, dtype=torch.float32),\n","    )\n","    train_loader = torch.utils.data.DataLoader(train_ds, batch_size=int(params[\"batch_size\"]), shuffle=True)\n","    val_loader = torch.utils.data.DataLoader(val_ds, batch_size=128, shuffle=False)\n","\n","    model = ResNet10(num_tasks=7) # Changed to ResNet10\n","    if torch.cuda.device_count() > 1:\n","        model = nn.DataParallel(model)\n","    model.to(device)\n","\n","    optimizer = ProxGEN(model.parameters(), lr=params[\"lr\"], lam=params[\"lam\"])\n","    criterion = nn.MSELoss()\n","\n","    for epoch in range(30):\n","        model.train()\n","        for xb, yb in train_loader:\n","            xb, yb = xb.to(device), yb.to(device)\n","            pred = model(xb)\n","            loss = criterion(pred, yb)\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","    model.eval()\n","    preds, labels = [], []\n","    with torch.no_grad():\n","        for xb, yb in val_loader:\n","            xb, yb = xb.to(device), yb.to(device)\n","            pred = model(xb)\n","            preds.append(pred.cpu().numpy())\n","            labels.append(yb.cpu().numpy())\n","    preds, labels = np.vstack(preds), np.vstack(labels)\n","    mse = mean_squared_error(labels, preds)\n","    return mse\n","\n","def objective(params):\n","    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","    mses = Parallel(n_jobs=-1)(delayed(train_and_eval)(\n","        X_trainval[train_idx], Y_trainval[train_idx],\n","        X_trainval[val_idx], Y_trainval[val_idx],\n","        params\n","    ) for train_idx, val_idx in kf.split(X_trainval))\n","    return {\"loss\": np.mean(mses), \"status\": STATUS_OK}\n","\n","# ----------------------------\n","# Bayesian Optimization\n","# ----------------------------\n","search_space = {\n","    \"lr\": hp.loguniform(\"lr\", np.log(1e-4), np.log(1e-2)),\n","    \"lam\": hp.loguniform(\"lam\", np.log(1e-3), np.log(1e+2)),\n","    \"batch_size\": hp.choice(\"batch_size\", [32, 64, 128]),\n","}\n","\n","trials = Trials()\n","best = fmin(fn=objective, space=search_space, algo=tpe.suggest,\n","            max_evals=50, trials=trials, rstate=np.random.default_rng(42))\n","\n","print(\"Best hyperparameters:\", best)\n","print(\"Best batch size:\", [32, 64, 128][best[\"batch_size\"]])\n","\n","# ----------------------------\n","# Final Training on train+val\n","# ----------------------------\n","final_model = ResNet10(num_tasks=7) # Changed to ResNet10\n","if torch.cuda.device_count() > 1:\n","    final_model = nn.DataParallel(final_model)\n","final_model.to(device)\n","\n","final_optimizer = ProxGEN(final_model.parameters(),\n","                          lr=best[\"lr\"], lam=best[\"lam\"])\n","criterion = nn.MSELoss()\n","\n","trainval_ds = torch.utils.data.TensorDataset(\n","    torch.tensor(X_trainval, dtype=torch.float32).unsqueeze(1),\n","    torch.tensor(Y_trainval, dtype=torch.float32),\n",")\n","trainval_loader = torch.utils.data.DataLoader(trainval_ds,\n","                                              batch_size=[32,64,128][best[\"batch_size\"]],\n","                                              shuffle=True)\n","\n","for epoch in range(50):\n","    final_model.train()\n","    for xb, yb in trainval_loader:\n","        xb, yb = xb.to(device), yb.to(device)\n","        pred = final_model(xb)\n","        loss = criterion(pred, yb)\n","        final_optimizer.zero_grad()\n","        loss.backward()\n","        final_optimizer.step()\n","\n","# ----------------------------\n","# Test Evaluation and Metrics Calculation\n","# ----------------------------\n","test_ds = torch.utils.data.TensorDataset(\n","    torch.tensor(X_test, dtype=torch.float32).unsqueeze(1),\n","    torch.tensor(Y_test, dtype=torch.float32),\n",")\n","test_loader = torch.utils.data.DataLoader(test_ds, batch_size=128, shuffle=False)\n","\n","final_model.eval()\n","preds, labels = [], []\n","with torch.no_grad():\n","    for xb, yb in test_loader:\n","        xb, yb = xb.to(device), yb.to(device)\n","        pred = final_model(xb)\n","        preds.append(pred.cpu().numpy())\n","        labels.append(yb.cpu().numpy())\n","preds, labels = np.vstack(preds), np.vstack(labels)\n","\n","# Calculate and print total MSE and R2\n","total_mse = mean_squared_error(labels, preds)\n","r2_per_trait = r2_score(labels, preds, multioutput=\"raw_values\")\n","print(\"\\n--- Final Test Evaluation ---\")\n","print(f\"Total Test MSE: {total_mse:.4f}\")\n","print(\"R2 per trait:\", r2_per_trait)\n","\n","# Calculate and print MSE per trait\n","mse_per_trait = mean_squared_error(labels, preds, multioutput='raw_values')\n","print(\"Mean Test MSE for each trait:\", mse_per_trait)\n","\n","# Calculate and print Pearson correlation coefficient\n","correlations = calculate_correlations(labels, preds)\n","print(\"Pearson correlation coefficient per trait:\", correlations)\n","\n","# Calculate and print sparsity\n","if isinstance(final_model, nn.DataParallel):\n","    sparsity = calculate_sparsity(final_model.module)\n","else:\n","    sparsity = calculate_sparsity(final_model)\n","print(f\"Model sparsity: {sparsity:.2f}%\")"]},{"cell_type":"code","source":[],"metadata":{"id":"RcU1xEKbVDcj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"e4TSMVbsVDMD"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOIpIQ6i9CbQyZvizLjF+LT"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}