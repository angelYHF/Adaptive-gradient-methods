{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOwDDjEA6zI3Gej1pXLXEAO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"o99cIeggktg0"},"outputs":[],"source":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split, KFold\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import mean_squared_error, r2_score\n","from hyperopt import hp, fmin, tpe, Trials, STATUS_OK\n","from joblib import Parallel, delayed\n","import os\n","from scipy.stats import pearsonr\n","\n","# ----------------------------\n","# Proximal operator for MCP\n","# ----------------------------\n","def prox_mcp(v, alpha, a=2.0):\n","    \"\"\"\n","    Proximal operator for MCP regularization.\n","\n","    alpha corresponds to the lambda * step_size term.\n","    a is the MCP parameter.\n","    \"\"\"\n","    abs_v = torch.abs(v)\n","\n","    # Calculate the threshold based on the preconditioner update rule\n","    threshold = alpha\n","\n","    # The two main thresholds for the piecewise function\n","    cond1 = abs_v <= threshold\n","    cond2 = (abs_v > threshold) & (abs_v <= a * threshold)\n","\n","    # Initialize the result tensor with zeros\n","    result = torch.zeros_like(v)\n","\n","    # Case 1: |v| <= lambda\n","    result[cond1] = 0.0\n","\n","    # Case 2: lambda < |v| <= a * lambda\n","    if cond2.any():\n","        v_cond2 = v[cond2]\n","        abs_v_cond2 = abs_v[cond2]\n","\n","        # Apply the shrinkage formula for the intermediate coefficients\n","        term1 = torch.sign(v_cond2) * (abs_v_cond2 - alpha)\n","        result[cond2] = term1 / (1 - 1/a)\n","\n","    # Case 3: |v| > a * lambda\n","    # This case does not require any calculation, as the value remains unchanged\n","    result[abs_v > a * threshold] = v[abs_v > a * threshold]\n","\n","    return result\n","\n","# ----------------------------\n","# Adaptive proximal gradient optimizer (based on Adam)\n","# ----------------------------\n","class ProxGEN(optim.Optimizer):\n","    \"\"\"\n","    Adaptive proximal gradient optimizer (based on Adam) for MCP regularization.\n","    \"\"\"\n","    def __init__(self, params, lr=1e-3, lam=1e-6, betas=(0.1, 0.999), eps=1e-8, mcp_a=2.0):\n","        defaults = dict(lr=lr, lam=lam, betas=betas, eps=eps, mcp_a=mcp_a)\n","        super(ProxGEN, self).__init__(params, defaults)\n","\n","    @torch.no_grad()\n","    def step(self, closure=None):\n","        loss = None\n","        if closure is not None:\n","            with torch.enable_grad():\n","                loss = closure()\n","\n","        for group in self.param_groups:\n","            for p in group[\"params\"]:\n","                if p.grad is None:\n","                    continue\n","                grad = p.grad.data\n","                state = self.state[p]\n","\n","                if len(state) == 0:\n","                    state[\"step\"] = 0\n","                    state[\"exp_avg\"] = torch.zeros_like(p)\n","                    state[\"exp_avg_sq\"] = torch.zeros_like(p)\n","\n","                exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n","                beta1, beta2 = group[\"betas\"]\n","\n","                state[\"step\"] += 1\n","                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n","                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n","\n","                denom = exp_avg_sq.sqrt().add_(group[\"eps\"])\n","                step_size = group[\"lr\"]\n","\n","                # Perform the Adam-like update\n","                theta_hat = p.data.addcdiv(exp_avg, denom, value=-step_size)\n","\n","                # The alpha term for the proximal operator\n","                alpha_prox = group[\"lam\"] * step_size\n","                mcp_a = group[\"mcp_a\"]\n","\n","                # Proximal step for MCP regularization\n","                p.data = prox_mcp(theta_hat, alpha_prox, mcp_a)\n","\n","        return loss\n","\n","# ----------------------------\n","# Simple Linear Model\n","# ----------------------------\n","class LinearModel(nn.Module):\n","    def __init__(self, input_dim, output_dim):\n","        super(LinearModel, self).__init__()\n","        self.linear = nn.Linear(input_dim, output_dim)\n","\n","    def forward(self, x):\n","        x = x.view(x.size(0), -1)\n","        return self.linear(x)\n","\n","# ----------------------------\n","# Metrics Calculation\n","# ----------------------------\n","def calculate_sparsity(model):\n","    \"\"\"Calculates the sparsity of the model's weights.\"\"\"\n","    total_params = 0\n","    non_zero_params = 0\n","    for param in model.parameters():\n","        if param.dim() > 1:\n","            total_params += param.numel()\n","            non_zero_params += torch.count_nonzero(param.data).item()\n","    if total_params == 0:\n","        return 0.0\n","    sparsity = (1 - (non_zero_params / total_params)) * 100\n","    return sparsity\n","\n","def calculate_correlations(labels, predictions):\n","    \"\"\"Calculates the Pearson correlation coefficient for each trait.\"\"\"\n","    num_tasks = labels.shape[1]\n","    correlations = []\n","    for i in range(num_tasks):\n","        corr, _ = pearsonr(labels[:, i], predictions[:, i])\n","        correlations.append(corr)\n","    return correlations\n","\n","# ----------------------------\n","# Load Data\n","# ----------------------------\n","data = pd.read_csv(\"/content/drive/My Drive/Adaptive gradient method/Adaptive gradient method from L_0 to L infnity/Final_pine_data.csv\")\n","Y = data.iloc[:, :7].values\n","X = data.iloc[:, 7:].values\n","scaler = StandardScaler()\n","X = scaler.fit_transform(X)\n","X_trainval, X_test, Y_trainval, Y_test = train_test_split(\n","    X, Y, test_size=0.2, random_state=42\n",")\n","input_dim = X.shape[1]\n","num_tasks = Y.shape[1]\n","\n","# ----------------------------\n","# Training + CV Evaluation\n","# ----------------------------\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","def train_and_eval(X_train, Y_train, X_val, Y_val, params):\n","    train_ds = torch.utils.data.TensorDataset(\n","        torch.tensor(X_train, dtype=torch.float32),\n","        torch.tensor(Y_train, dtype=torch.float32),\n","    )\n","    val_ds = torch.utils.data.TensorDataset(\n","        torch.tensor(X_val, dtype=torch.float32),\n","        torch.tensor(Y_val, dtype=torch.float32),\n","    )\n","    train_loader = torch.utils.data.DataLoader(train_ds, batch_size=int(params[\"batch_size\"]), shuffle=True)\n","    val_loader = torch.utils.data.DataLoader(val_ds, batch_size=128, shuffle=False)\n","\n","    model = LinearModel(input_dim, num_tasks).to(device)\n","    optimizer = ProxGEN(model.parameters(), lr=params[\"lr\"], lam=params[\"lam\"])\n","    criterion = nn.MSELoss()\n","\n","    for epoch in range(30):\n","        model.train()\n","        for xb, yb in train_loader:\n","            xb, yb = xb.to(device), yb.to(device)\n","            pred = model(xb)\n","            loss = criterion(pred, yb)\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","    model.eval()\n","    preds, labels = [], []\n","    with torch.no_grad():\n","        for xb, yb in val_loader:\n","            xb, yb = xb.to(device), yb.to(device)\n","            pred = model(xb)\n","            preds.append(pred.cpu().numpy())\n","            labels.append(yb.cpu().numpy())\n","    preds, labels = np.vstack(preds), np.vstack(labels)\n","    mse = mean_squared_error(labels, preds)\n","    return mse\n","\n","def objective(params):\n","    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","    mses = Parallel(n_jobs=-1)(delayed(train_and_eval)(\n","        X_trainval[train_idx], Y_trainval[train_idx],\n","        X_trainval[val_idx], Y_trainval[val_idx],\n","        params\n","    ) for train_idx, val_idx in kf.split(X_trainval))\n","    return {\"loss\": np.mean(mses), \"status\": STATUS_OK}\n","\n","# ----------------------------\n","# Bayesian Optimization\n","# ----------------------------\n","search_space = {\n","    \"lr\": hp.loguniform(\"lr\", np.log(1e-4), np.log(1e-2)),\n","    \"lam\": hp.loguniform(\"lam\", np.log(1e-3), np.log(1e+2)),\n","    \"batch_size\": hp.choice(\"batch_size\", [32, 64, 128]),\n","}\n","\n","trials = Trials()\n","best = fmin(fn=objective, space=search_space, algo=tpe.suggest,\n","            max_evals=50, trials=trials, rstate=np.random.default_rng(42))\n","\n","print(\"Best hyperparameters:\", best)\n","print(\"Best batch size:\", [32, 64, 128][best[\"batch_size\"]])\n","\n","# ----------------------------\n","# Final Training on train+val\n","# ----------------------------\n","final_model = LinearModel(input_dim, num_tasks).to(device)\n","final_optimizer = ProxGEN(final_model.parameters(), lr=best[\"lr\"], lam=best[\"lam\"])\n","criterion = nn.MSELoss()\n","\n","trainval_ds = torch.utils.data.TensorDataset(\n","    torch.tensor(X_trainval, dtype=torch.float32),\n","    torch.tensor(Y_trainval, dtype=torch.float32),\n",")\n","trainval_loader = torch.utils.data.DataLoader(trainval_ds,\n","                                              batch_size=[32, 64, 128][best[\"batch_size\"]],\n","                                              shuffle=True)\n","\n","for epoch in range(50):\n","    final_model.train()\n","    for xb, yb in trainval_loader:\n","        xb, yb = xb.to(device), yb.to(device)\n","        pred = final_model(xb)\n","        loss = criterion(pred, yb)\n","        final_optimizer.zero_grad()\n","        loss.backward()\n","        final_optimizer.step()\n","\n","# ----------------------------\n","# Test Evaluation and Metrics Calculation\n","# ----------------------------\n","test_ds = torch.utils.data.TensorDataset(\n","    torch.tensor(X_test, dtype=torch.float32),\n","    torch.tensor(Y_test, dtype=torch.float32),\n",")\n","test_loader = torch.utils.data.DataLoader(test_ds, batch_size=128, shuffle=False)\n","\n","final_model.eval()\n","preds, labels = [], []\n","with torch.no_grad():\n","    for xb, yb in test_loader:\n","        xb, yb = xb.to(device), yb.to(device)\n","        pred = final_model(xb)\n","        preds.append(pred.cpu().numpy())\n","        labels.append(yb.cpu().numpy())\n","preds, labels = np.vstack(preds), np.vstack(labels)\n","\n","# Calculate and print total MSE and R2\n","total_mse = mean_squared_error(labels, preds)\n","r2_per_trait = r2_score(labels, preds, multioutput=\"raw_values\")\n","print(\"\\n--- Final Test Evaluation ---\")\n","print(f\"Total Test MSE: {total_mse:.4f}\")\n","print(\"R2 per trait:\", r2_per_trait)\n","\n","# Calculate and print MSE per trait\n","mse_per_trait = mean_squared_error(labels, preds, multioutput='raw_values')\n","print(\"Mean Test MSE for each trait:\", mse_per_trait)\n","\n","# Calculate and print Pearson correlation coefficient\n","correlations = calculate_correlations(labels, preds)\n","print(\"Pearson correlation coefficient per trait:\", correlations)\n","\n","# Calculate and print sparsity\n","sparsity = calculate_sparsity(final_model)\n","print(f\"Model sparsity: {sparsity:.2f}%\")"],"metadata":{"id":"6j2rutXkkwSw"},"execution_count":null,"outputs":[]}]}